{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b1e293c",
   "metadata": {},
   "source": [
    "recap: previously in 2025:\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2593e557",
   "metadata": {},
   "source": [
    "MATCHING GENEFORMER\n",
    "- redo global normalization code; its not using the normalized data\n",
    "- show entropy, etc. plots of rank matrix after normalization; shouldn't be the U-shape if its been normalized like that... right?\n",
    "- need to accordingly normalize the input for the expression model too for comparison\n",
    "- can do learned posenc, however this likely will not make a difference (carl says there is literature on it not affecting much)\n",
    "- likely tho this won't be the primary cause of the issue\n",
    "\n",
    "SEQUENTIAL VS. TABULAR\n",
    "- verify what is tabular? what is sequential?\n",
    "- from these results, we can conclude that it is being seen as sequential (specifically due to CLS embeddign plot)\n",
    "- any way to make it more understandable? might not be relevant tho\n",
    "- maybe try concat instead of addition and see if that changes anything lol\n",
    "\n",
    "EVALUATING PREDICTIONS\n",
    "- need *** plot variance by average expression (0-15) to see if it matches the distribution of the variance by rank plot\n",
    "- can conclude from the results that the hypothesis is true, however what lea says is that the variance by rank plot is incorrect?\n",
    "\n",
    "LINCS VS. TCGA\n",
    "- can do TCGA because this is more similar to actual RNA seq we will be using later\n",
    "- if there is a discrepancy between LINCS and TCGA resutls, then the result is readout-specific\n",
    "- don't need to test EVERYTHING on tcga, just the results that are noteworthy from LINCS\n",
    "\n",
    "INCORPORATING EXP VALUES\n",
    "- carl asked: isn't the point of doing rank so that we don't have to use expression values??\n",
    "\n",
    "for seb meeting:\n",
    "- suggestions for CP presentation?\n",
    "- what can i be doing better as a student/researcher? (maybe aritculation...)\n",
    "- i feel behind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc3d3f6",
   "metadata": {},
   "source": [
    "jan 18, 2025 - post-CP reorganization + hybrid research\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b58cff",
   "metadata": {},
   "source": [
    "RE: PREV. RESULTS\n",
    "- re-run all 4 results on trt dataset; make sure to match AE and TF # params\n",
    "- clean up and separate files (esp AE)\n",
    "- ?clean up GF testing code?\n",
    "\n",
    "RE: HYBRID\n",
    "- lit review on hybrid methods\n",
    "- benchmarking tx eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3a47fa",
   "metadata": {},
   "source": [
    "jan 23, 2025 - re-running reformatted code + more hybrid research\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaac88e",
   "metadata": {},
   "source": [
    "WRT RESULTS\n",
    "\n",
    "- rank_tf --> oni via sbatch (1ep) - 6hrs w/ batchsize of 210\n",
    "    n_epochs = ?\n",
    "    embed_dim = 128\n",
    "    hidden_dim = 256\n",
    "    n_heads = 2\n",
    "    n_layers = 4\n",
    "    drop_prob = 0.05\n",
    "    lr = 0.001\n",
    "    mask_ratio = 0.1\n",
    "    - untrt -> 15min on smaug (128), 12min on oni (210) ? \n",
    "\n",
    "- exp_tf --> running rn on oni via sbatch (1ep)\n",
    "\n",
    "\n",
    "- rank_ae --> 52min on smaug via sbatch (1ep)\n",
    "    n_epochs = ?\n",
    "    latent_1 = \n",
    "    latent_2 = 225\n",
    "    latent_3 = 150\n",
    "    embed_dim = 128\n",
    "    drop_prob = 0.05\n",
    "    lr = 0.001\n",
    "    mask_ratio = 0.1\n",
    "\n",
    "- exp_ae --> 1min on oni via sbatch (1ep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b4c069",
   "metadata": {},
   "source": [
    "WRT CODE\n",
    "\n",
    "- ensure rank ae is done properly\n",
    "- only thing is the \"nothing\"s that COULD be changed to work better with the file but not tooo important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfd6eb6",
   "metadata": {},
   "source": [
    "WRT RESEARCH\n",
    "\n",
    "- looking into: scVI, PCA, Harmony, HVG; how can what works with these be combined with our tf embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0c9342",
   "metadata": {},
   "source": [
    "jan 27, 2026 - some notes on using calcul quebec\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59275e6",
   "metadata": {},
   "source": [
    "ON LOGIN NODE:\n",
    "- ssh chans@rorqual.alliancecan.ca\n",
    "- julia --project=/home/chans/links/scratch/lincsv3 -e 'using Pkg; Pkg.instantiate(); Pkg.precompile()'\n",
    "- (there are some issues with module stuff, may need module load cuda/12.9 julia/1.11.3?)\n",
    "\n",
    "TO ALLOCATE:\n",
    "- sbatch --time=00:30:00 simple_job.sh \n",
    "- salloc --time=1:0:0 --mem-per-cpu=3G --ntasks=1 --account=def-someuser  \n",
    "    - salloc --time=00:30:00 --cpus-per-task=8 --gpus=nvidia_h100_80gb_hbm3_3g.40gb:1 --mem=62G\n",
    "\n",
    "SLURM COMMANDS:\n",
    "- sq: to check squeue for -u chans\n",
    "- scontrol show job 632661 | grep: to see the path to executable/script\n",
    "- scontrol show job 12345: for full info\n",
    "- scontrol write batch_script 12345: for the file itself\n",
    "\n",
    "NOHUP COMMANDS:\n",
    "- ps -fp 12345: for script\n",
    "- ps -p 12345 -o args: for wrap\n",
    "- cat /proc/12345/cmdline: for all running processes\n",
    "- ls -l /proc/12345/cwd: for folder\n",
    "\n",
    "STEPS:\n",
    "- ssh rorqual\n",
    "- export JULIA_PKG_PRECOMPILE_AUTO=0\n",
    "- instantiate pkgs, ensure everything is pkg.added\n",
    "    - julia --project=/home/chans/links/scratch/lincsv3 -e 'using Pkg; Pkg.instantiate(); Pkg.precompile()'\n",
    "- salloc\n",
    "    - salloc --time=00:60:00 --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=124G --gpus=h100:1\n",
    "- module load julia/1.11.3 cuda/12.2 cudnn\n",
    "- julia --project=/home/chans/links/scratch/lincsv3\n",
    "    - cd to scripts/\n",
    "    - julia --project=../\n",
    "- ctrl + shift + enter to run by line in REPL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ce19e8",
   "metadata": {},
   "source": [
    "batchsizes w/ rorqual:\n",
    "testing: 340"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd853c5b",
   "metadata": {},
   "source": [
    "feb 5, 2026 - btiming/memory allocs :P\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791a31c2",
   "metadata": {},
   "source": [
    "given results.txt, #TODO:\n",
    "- main issue: cpu-gpu movement\n",
    "- also:\n",
    "    - :gradient_calc, :loss_fwd, :m-transformer, :tf-attn\n",
    "    - :get_ranks/errors is performing scalar iteration on CPU\n",
    "        - moves the entire logit matrix to CPU and sorts preds for every single sample sequentially w/ loop; GPU idles\n",
    "        - sol'n: instead of sorting on the CPU, calculating the rank of a target can be done via a broadcast so no need to sortperm\n",
    "\n",
    "re: CPU/GPU data transfer:\n",
    "- 36.5% of CUDA API: cudaMemcpyAsync (blocking bc of pageable memory)\n",
    "    - sol'n: pinned array? (via CUDA.pin())\n",
    "- 85.1% of GPU memory: device-to-host (gpu to cpu)\n",
    "    - logits_masked and y_masked to CPU at the end of every batch in the test loop\n",
    "    - sol'n: maybe dont need Flux.onecold?\n",
    "\n",
    "https://cuda.juliagpu.org/stable/usage/memory/ \n",
    "http://fluxml.ai/Flux.jl/stable/guide/performance/ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb9d934",
   "metadata": {},
   "source": [
    "feb 9, 2026 - start merging embeddings!!!\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb92b87f",
   "metadata": {},
   "source": [
    "step 1: hybrid\n",
    "- seb's approach: either:\n",
    "    1. CLS token = PCA_emb --> transformer\n",
    "    2. CLS token + PCA_emb --> transformer\n",
    "    3. transformer --> trained CLS token --> PCA_emb --> transformer\n",
    "- my approach: ranks_emb + encoder_emb --> transformer (reg/graph)\n",
    "\n",
    "step 2: benchmark\n",
    "- dataset: currently LINCS, could consider Lea's Tahoe pseudobulk\n",
    "- tasks:\n",
    "    - easy: identify cell line\n",
    "    - medium: treated vs. untreated given profile\n",
    "    - medium-hard: predict gene signature close to a phenotype\n",
    "    - hard: signature activation; if a compound will induce some activation pathway\n",
    "    - extra-hard: delta profiles (see Lea)\n",
    "- via fine-tuning:\n",
    "    - pretrain --> freeze --> add classifier head --> train --> take whole model --> train again = finetuning?\n",
    "\n",
    "notes:\n",
    "- log10(FLOPS) instead of epoch?\n",
    "- emb_dim = 512\n",
    "- see scFoundation embedding abalation in supp. material after having tried both hybrid approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb4a07a",
   "metadata": {},
   "source": [
    "step 1: approach #1 (CLS = PCA_emb for tf input)\n",
    "- generate PCA vector for entire dataset\n",
    "    - decide on #PCs\n",
    "    - MultivariateStats.jl\n",
    "    - project samples down to PC_dim\n",
    "    - normalize x ~ N(0,1)\n",
    "- architecture\n",
    "    - need to project PCA vector size to transformer embedding input size\n",
    "    - then embed the rest of the sequence aside from the CLS\n",
    "- fwd pass\n",
    "    - stick this CLS in front of batched embedding input\n",
    "\n",
    "- how it works:\n",
    "    - input: X_train (ranked matrix of n_genes x n_samples)\n",
    "    - apply PCA for dim=64; pca_train is proj matrix of 64 x n_genes\n",
    "    - masking applied to X_train\n",
    "    - in training loop:\n",
    "        - pca_train changed for each batch:\n",
    "            - all mask_id converted into Float32 (0.0)\n",
    "            - pca_train projected into batchsize (64 x n_genes --> 64 x batch_size)\n",
    "            - z-score normalization\n",
    "        - fwd pass:\n",
    "            - move to gpu\n",
    "\n",
    "            - input --> embedding (embed_dim, n_genes, batch_size)\n",
    "            - pca_train --> projected to embed_dim (embed_dim, batch_size)\n",
    "            - concat input + pca_train (embed_dim, n_genes + 1,  batch_size)\n",
    "\n",
    "            - posenc (sin/cos) summation to input\n",
    "            - pass thru transformer; PCA token at idx 1 aggregates info from all genes and allows all other gene tokens to see global variance (pca) context?\n",
    "            - classification projects emb_dim to n_classes for logits prediction step\n",
    "            - loss calculation: LCE\n",
    "            \n",
    "            - during backprop, gradient updates model.embedding separately and model.pca_proj separately\n",
    "                - gradients do not flow into pca_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffefcd9",
   "metadata": {},
   "source": [
    "step 1: approach #2 (CLS +/concat PCA_emb for tf input)\n",
    "- as previously, take PCA vector but instead of just replacing CLS with the projected PCs, we can add or concat the PCA projection to the learned CLS (thus having 2 sets of learned weights as opposed to 1!)\n",
    "\n",
    "- how it works:\n",
    "    - input: X_train (ranked matrix of n_genes x n_samples)\n",
    "    - apply PCA for dim=64; pca_train is proj matrix of 64 x n_genes\n",
    "    - masking applied to X_train\n",
    "    - in training loop:\n",
    "        - pca_train changed for each batch:\n",
    "            - all mask_id converted into Float32 (0.0)\n",
    "            - pca_train projected into batchsize (64 x n_genes --> 64 x batch_size)\n",
    "            - z-score normalization\n",
    "        - fwd pass:\n",
    "            - move to gpu\n",
    "\n",
    "            - input --> embedding (embed_dim, n_genes, batch_size)\n",
    "            - pca_train --> projected to embed_dim (embed_dim, batch_size)\n",
    "            - cls_emb = cls_token.weight (learnable single vector initialized randomly to rep. cell-wise emb)\n",
    "            - hybrid = pca_emb .+ cls_emb\n",
    "            - concat input + hybrid_emb (embed_dim, n_genes + 1,  batch_size)\n",
    "\n",
    "            - posenc (sin/cos) summation to input\n",
    "            - pass thru transformer; hybrid token at idx 1 aggregates info from all genes and allows all other gene tokens to see global variance AND baseline ranked embedding context?\n",
    "            - classification projects emb_dim to n_classes for logits prediction step\n",
    "            - loss calculation: LCE\n",
    "            \n",
    "            - during backprop, gradient updates model.embedding separately and model.pca_proj and model.cls_token separately\n",
    "                - gradients do not flow into pca_train\n",
    "                - model.embedding: vector rep of specific genes\n",
    "                - model.cls_token: vector rep of specific cells\n",
    "                - model.pca_proj: vector rep of PCA statistics (shift from baseline cls)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743889da",
   "metadata": {},
   "source": [
    "feb 11, 2026 - currently running:\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ab2569",
   "metadata": {},
   "source": [
    "using trt:\n",
    "version 1: smaug 10ep w/ batchsize=128 + smaug 50ep w/ batchsize=128\n",
    "version 2: kraken 1PM; n_ep=10, batch_size=42 on nohup (761928)\n",
    "\n",
    "regular rtf: smaug 50ep w/ batchsize=128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c925a57",
   "metadata": {},
   "source": [
    "feb 17, 2026 - \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150e1f7f",
   "metadata": {},
   "source": [
    "currently have:\n",
    "version 2: kraken 1PM; n_ep=10, batch_size=42 on nohup (761928) - trt\n",
    "\n",
    "running:\n",
    "version 1: smaug 10ep w/ batchsize=128 - trt\n",
    "\n",
    "\n",
    "if no time, run:\n",
    "\n",
    "PARAMETERS:\n",
    "########### kraken\n",
    "dataset = untrt\n",
    "masking_ratio = 0.15\n",
    "mask_token_id = 979\n",
    "batch_size = 64\n",
    "n_epochs = 30\n",
    "embed_dim = 128\n",
    "latent_1 = 450\n",
    "latent_2 = 225\n",
    "latent_3 = 110\n",
    "learning_rate = 0.001\n",
    "ADDITIONAL NOTES: test run with new onehot input\n",
    "run_time = 9 hours and 17 minutes\n",
    "final_accuracy = 0.011015662717221921\n",
    "\n",
    "for regular rtf, then v1 or v2 (whcihever u wanna explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3184afe5",
   "metadata": {},
   "source": [
    "feb 23, 2026 - reorganize, redo PCA implementation\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7085bfc",
   "metadata": {},
   "source": [
    "for reference, do:\n",
    "\n",
    "*to create shell attached to sbatch (because regular sbatch is weird sometimes)\n",
    "tmux new -s v1\n",
    "srun --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=90G --gres=gpu:V100:1 --time=10:00:00 --pty bash -i\n",
    "nohup julia scripts/hybrid/version1.jl > v1_rtf_10ep.log 2>&1 &\n",
    "\n",
    "*to reattach after ending session\n",
    "tmux ls\n",
    "tmux attach -t v1\n",
    "\n",
    "*to delete session\n",
    "tmux kill-session -t v1\n",
    "tmux kill-server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c0b221",
   "metadata": {},
   "source": [
    "to run for good plot comparison:\n",
    "- rtf: trt, 50ep\n",
    "    - 1ep: r(smaug-tmux)\n",
    "- rae: trt, 50ep\n",
    "    - 1ep: r(smaug-tmux)\n",
    "- etf: trt, 50ep\n",
    "    - 1ep: r(smaug-gpu1)\n",
    "- eae: trt, 50ep\n",
    "    - 1ep: 2mins (128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6618da82",
   "metadata": {},
   "source": [
    "for pca implementation. to run:\n",
    "- no PCA no CLS (regular rank_ae.jl)\n",
    "- PCA via concat (so PCA == CLS) (version 1)\n",
    "- PCA via sum (exact same PCA per sample is summed to each row's embedding vector) (fix - version 2)\n",
    "\n",
    "(since it was also said by quentin that CLS can be replaced with just taking the mean across all embedding vectors for each sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d747de6",
   "metadata": {},
   "source": [
    "additional hybrids to run for good plot comparison:\n",
    "- version 1: trt, 50ep\n",
    "    - 1ep:\n",
    "- version 2: trt, 50ep\n",
    "    - 1ep:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
